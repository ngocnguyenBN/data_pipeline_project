from cassandra.cluster import Cluster
import pandas as pd
import concurrent.futures
from cassandra.concurrent import execute_concurrent_with_args


import subprocess

cql_script_local = "D:/data_pipeline_project/app/database/cassandra_schema.cql"
cql_script_container = "/cassandra_schema.cql"
cassandra_container = "cassandra-1"  # T√™n container Cassandra trong Docker

# üîπ 1. Copy file `.cql` t·ª´ m√°y host v√†o container Docker
subprocess.run(
    ["docker", "cp", cql_script_local, f"{cassandra_container}:{cql_script_container}"],
    check=True,
)

# üîπ 2. Ch·∫°y cqlsh b√™n trong container ƒë·ªÉ th·ª±c thi script
subprocess.run(
    [
        "docker",
        "exec",
        "-i",
        cassandra_container,
        "cqlsh",
        "172.19.0.3",
        "-f",
        cql_script_container,
    ],
    check=True,
)

print("‚úÖ CQL script executed successfully inside Docker!")

print("‚úÖ CQL script executed successfully!")

# K·∫øt n·ªëi t·ªõi Cassandra
cluster = Cluster(["localhost"], port=9042)
session = cluster.connect()

# S·ª≠ d·ª•ng keyspace
session.set_keyspace("my_keyspace")

# Chu·∫©n b·ªã c√¢u l·ªánh insert
insert_query = session.prepare(
    """
    INSERT INTO imbalance_data (
        ticker, code, date, source, codesource, close_und, sharesout, name,
        group_by, code_und, name_und, close, category, size, market, updated, type
    ) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
"""
)

# ƒê·ªçc d·ªØ li·ªáu CSV
df = pd.read_csv(
    "./resources/data_imbalance.csv",
    dtype={"category": str},
    parse_dates=[
        "date",
        "updated",
    ],  # Chuy·ªÉn ƒë·ªïi sang datetime ƒë·ªÉ ph√π h·ª£p v·ªõi Cassandra
    low_memory=False,
)

# Chia nh·ªè d·ªØ li·ªáu
batch_size = 5000
df_batches = [df[i : i + batch_size] for i in range(0, len(df), batch_size)]


# # H√†m insert batch t·ªëi ∆∞u h∆°n (gi·ªØ nguy√™n c√°ch c≈©)
# def insert_batch(batch):
#     for _, row in batch.iterrows():
#         try:
#             # Chuy·ªÉn ƒë·ªïi ki·ªÉu d·ªØ li·ªáu ƒë√∫ng v·ªõi Cassandra
#             date_value = row["date"].date() if pd.notna(row["date"]) else None
#             updated_value = (
#                 row["updated"].to_pydatetime() if pd.notna(row["updated"]) else None
#             )
#             size_value = str(row["size"]) if pd.notna(row["size"]) else None

#             future = session.execute_async(
#                 insert_query,
#                 (
#                     row["ticker"],
#                     row["code"],
#                     date_value,
#                     row["source"],
#                     row["codesource"],
#                     row["close_und"],
#                     row["sharesout"],
#                     row["name"],
#                     row["group_by"],
#                     row["code_und"],
#                     row["name_und"],
#                     row["close"],
#                     row["category"],
#                     size_value,
#                     row["market"],
#                     updated_value,
#                     row["type"],
#                 ),
#             )
#             future.result()  # Ch·ªù k·∫øt qu·∫£ ho√†n th√†nh ƒë·ªÉ tr√°nh l·ªói

#         except Exception as e:
#             print(f"‚ùå L·ªói khi insert {row['ticker']} v√†o {row['date']}: {e}")


# # D√πng ThreadPoolExecutor ƒë·ªÉ x·ª≠ l√Ω song song
# with concurrent.futures.ThreadPoolExecutor(max_workers=4) as executor:
#     executor.map(insert_batch, df_batches)


# T·∫°o danh s√°ch c√°c truy v·∫•n insert
insert_queries = [
    (
        insert_query,
        (
            row["ticker"],
            row["code"],
            row["date"].date(),
            row["source"],
            row["codesource"],
            row["close_und"],
            row["sharesout"],
            row["name"],
            row["group_by"],
            row["code_und"],
            row["name_und"],
            row["close"],
            row["category"],
            str(row["size"]),
            row["market"],
            row["updated"].to_pydatetime(),
            row["type"],
        ),
    )
    for _, row in df.iterrows()
]

# Ch·∫°y insert song song v·ªõi t·ªëi ƒëa 10 lu·ªìng
execute_concurrent_with_args(session, insert_queries, concurrency=10)

print("‚úÖ D·ªØ li·ªáu ƒë√£ ƒë∆∞·ª£c insert th√†nh c√¥ng!")

# Ki·ªÉm tra d·ªØ li·ªáu
rows = session.execute("SELECT * FROM imbalance_data LIMIT 10")
for row in rows:
    print(row)

# ƒê√≥ng k·∫øt n·ªëi
cluster.shutdown()

# from cassandra.auth import PlainTextAuthProvider
# import uuid

# # py -3.11 -m venv venv
# # Define connection settings
# cassandra_host = "localhost"  # Docker runs it on localhost
# cassandra_port = 9042
# username = "admin"
# password = "admin"

# # Connect to the Cassandra cluster
# auth_provider = PlainTextAuthProvider(username, password)
# cluster = Cluster([cassandra_host], port=cassandra_port, auth_provider=auth_provider)
